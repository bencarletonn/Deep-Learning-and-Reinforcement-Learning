{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gravitar-code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u72Qu5Kyw3Kf"
      },
      "source": [
        "**Initialise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NdxzrE9wHlh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaee7f9f-b873-4ef2-9f94-bb99df416a44"
      },
      "source": [
        "# this is a Deep Q Learning (DQN) agent including prioritized experience replay memory and a target network\r\n",
        "# The DQN uses a 4-frame stack of greyscale 84x84 images as states, to learn movement and accomodate efficient learning\r\n",
        "# The architecture and parameters are based off the original DQN paper, https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\r\n",
        "# a rank-based (prioritized) experience replay is used, first discussed in https://arxiv.org/abs/1511.05952\r\n",
        "# due to RAM restrictions on colab we use a 50000 buffer limit. Preferbly this would be larger. Preferbly we would train for much longer (>10mil frames)\r\n",
        "# code inspired by https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb\r\n",
        "\r\n",
        "# imports\r\n",
        "import gym\r\n",
        "import math \r\n",
        "import collections\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "# hyperparameters\r\n",
        "learning_rate = 0.0001\r\n",
        "gamma         = 0.99\r\n",
        "buffer_limit  = 50000 \r\n",
        "batch_size    = 32\r\n",
        "video_every   = 25\r\n",
        "print_every   = 5\r\n",
        "save_model    = True  # Disable this if you don't want to save models to drive\r\n",
        "\r\n",
        "\r\n",
        "if save_model: \r\n",
        "    from google.colab import drive \r\n",
        "    drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIUPVSF0xbjX"
      },
      "source": [
        "**Gym Wrappers**\r\n",
        "Taken from https://github.com/higgsfield/RL-Adventure/blob/master/common/wrappers.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K83LJw8wxZv8"
      },
      "source": [
        "class FireResetEnv(gym.Wrapper):\r\n",
        "\tdef __init__(self, env=None):\r\n",
        "\t\t\"\"\"For environments where the user need to press FIRE for the\r\n",
        "\t\tgame to start.\"\"\"\r\n",
        "\t\tsuper(FireResetEnv, self).__init__(env)\r\n",
        "\t\tassert env.unwrapped.get_action_meanings()[1] == 'FIRE'\r\n",
        "\t\tassert len(env.unwrapped.get_action_meanings()) >= 3\r\n",
        "\r\n",
        "\tdef step(self, action):\r\n",
        "\t\treturn self.env.step(action)\r\n",
        "\r\n",
        "\tdef reset(self):\r\n",
        "\t\tself.env.reset()\r\n",
        "\t\tobs, _, done, _ = self.env.step(1)\r\n",
        "\t\tif done:\r\n",
        "\t\t\tself.env.reset()\r\n",
        "\t\tobs, _, done, _ = self.env.step(2)\r\n",
        "\t\tif done:\r\n",
        "\t\t\tself.env.reset()\r\n",
        "\t\treturn obs\r\n",
        "\r\n",
        "\r\n",
        "class NoopResetEnv(gym.Wrapper):\r\n",
        "    def __init__(self, env, noop_max=30):\r\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\r\n",
        "        No-op is assumed to be action 0.\r\n",
        "        \"\"\"\r\n",
        "        gym.Wrapper.__init__(self, env)\r\n",
        "        self.noop_max = noop_max\r\n",
        "        self.override_num_noops = None\r\n",
        "        self.noop_action = 0\r\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\r\n",
        "\r\n",
        "    def reset(self, **kwargs):\r\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\r\n",
        "        self.env.reset(**kwargs)\r\n",
        "        if self.override_num_noops is not None:\r\n",
        "            noops = self.override_num_noops\r\n",
        "        else:\r\n",
        "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101\r\n",
        "        assert noops > 0\r\n",
        "        obs = None\r\n",
        "        for _ in range(noops):\r\n",
        "            obs, _, done, _ = self.env.step(self.noop_action)\r\n",
        "            if done:\r\n",
        "                obs = self.env.reset(**kwargs)\r\n",
        "        return obs\r\n",
        "\r\n",
        "    def step(self, ac):\r\n",
        "        return self.env.step(ac)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-zYiIM-xeUV"
      },
      "source": [
        "**Model architecture + Prioritized experience**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF1qvRBHwOiA"
      },
      "source": [
        "class PrioritizedReplayBuffer():\r\n",
        "    def __init__(self, prob_alpha=0.6):\r\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\r\n",
        "        self.priorities = np.zeros((buffer_limit), dtype=np.float32)\r\n",
        "        self.pos = 0 \r\n",
        "        self.prob_alpha = prob_alpha\r\n",
        "\r\n",
        "    \r\n",
        "    def put(self, transition):\r\n",
        "        max_prio = self.priorities.max() if self.buffer else 1.0 # set to 1 if empty\r\n",
        "        self.buffer.append(transition) \r\n",
        "        self.priorities[self.pos] = max_prio\r\n",
        "        self.pos = (self.pos + 1) % buffer_limit\r\n",
        "\r\n",
        "\r\n",
        "    def sample(self, n, beta=0.4): # β = 0.4 \r\n",
        "        if len(self.buffer) == buffer_limit:\r\n",
        "            prios = self.priorities \r\n",
        "        else:\r\n",
        "            prios = self.priorities[:self.pos]\r\n",
        "         \r\n",
        "        probs = prios ** self.prob_alpha # p_{i}^{α}\r\n",
        "        probs = probs / probs.sum() # pr(i) = p_{i}^{α} / sum(p^{α})  \r\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs) \r\n",
        "        samples = [self.buffer[idx] for idx in indices]\r\n",
        "\r\n",
        "        total = len(self.buffer) \r\n",
        "        weights = (total * probs[indices]) ** (-beta)  # (N * pr(i))^β equivalent to (1/N * 1/pr(i))^β\r\n",
        "        weights = weights / weights.max() \r\n",
        "        weights = np.array(weights, dtype=np.float32) \r\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\r\n",
        "        for i in samples:\r\n",
        "            s_lst.append(i[0]) \r\n",
        "            a_lst.append([i[1]]) \r\n",
        "            r_lst.append([i[2]]) \r\n",
        "            s_prime_lst.append(i[3]) \r\n",
        "            done_mask_lst.append([i[4]])\r\n",
        "        \r\n",
        "        return torch.stack(s_lst), torch.tensor(a_lst), torch.tensor(r_lst), \\\r\n",
        "               torch.stack(s_prime_lst), torch.tensor(done_mask_lst), indices, weights \r\n",
        "    \r\n",
        "    def update_priorities(self, batch_indices, batch_priorities):\r\n",
        "        for idx, prio in zip(batch_indices, batch_priorities):  \r\n",
        "            self.priorities[idx] = max(prio)  \r\n",
        "   \r\n",
        "    def size(self):\r\n",
        "        return len(self.buffer)\r\n",
        "\r\n",
        "class QNetwork(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(QNetwork, self).__init__()\r\n",
        "\r\n",
        "        self.conv = nn.Sequential(\r\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "            nn.ReLU(),\r\n",
        "        )\r\n",
        "\r\n",
        "        self.linear = nn.Sequential(\r\n",
        "            nn.Linear(7*7*64, 512),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(512, env.action_space.n)\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv(x)\r\n",
        "        x = x.view(x.shape[0], -1) \r\n",
        "        x = self.linear(x) \r\n",
        "        return x \r\n",
        "      \r\n",
        "    def sample_action(self, obs, epsilon):\r\n",
        "        out = self.forward(obs)\r\n",
        "        coin = random.random()\r\n",
        "        if coin < epsilon:\r\n",
        "            return random.randint(0,env.action_space.n-1)\r\n",
        "        else : \r\n",
        "            return out.argmax().item()\r\n",
        "            \r\n",
        "def train(q, q_target, memory, optimizer, beta):\r\n",
        "    for i in range(10):\r\n",
        "\r\n",
        "        s,a,r,s_prime,done_mask,indices,weights = memory.sample(batch_size, beta)\r\n",
        "        s, s_prime = torch.squeeze(s), torch.squeeze(s_prime)\r\n",
        " \r\n",
        "        q_out = q(s)\r\n",
        "        q_a = q_out.gather(1,a)\r\n",
        "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\r\n",
        "        target = r + gamma * max_q_prime * done_mask\r\n",
        "        loss = (F.smooth_l1_loss(q_a, target, reduction='none')) * torch.from_numpy(weights)\r\n",
        "        prios = loss + 1e-5\r\n",
        "        loss = torch.mean(loss) \r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        memory.update_priorities(indices, prios.data.cpu().numpy())\r\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G8_K9YyxCWE"
      },
      "source": [
        "# Slowed down the rate of annealing to accomodate for slower learning. (training every 10 frames rather than every frame)\r\n",
        "\r\n",
        "# Epsilon\r\n",
        "epsilon_start = 1.0\r\n",
        "epsilon_final = 0.01\r\n",
        "epsilon_decay = 30000\r\n",
        "\r\n",
        "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * (frame_idx*0.1) / epsilon_decay)\r\n",
        "\r\n",
        "# Beta\r\n",
        "beta = 0.4 \r\n",
        "beta_start = 0.4\r\n",
        "beta_frames = 100000\r\n",
        "beta_by_frame = lambda frame_idx: min(1.0, beta_start + (frame_idx*0.1) * (1.0 - beta_start) / beta_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rxN2Yc9wyma"
      },
      "source": [
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r3PWe7qwmlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa14e82-e12b-478d-bad0-14be73f1fd78"
      },
      "source": [
        "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\r\n",
        "\r\n",
        "# setup the Gravitar ram environment, and record a video every 50 episodes.\r\n",
        "env = gym.make('Gravitar-v0')\r\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\r\n",
        "\r\n",
        "\r\n",
        "# reproducible environment and action spaces, do not change lines 6-11 here (tools > settings > editor > show line numbers)\r\n",
        "seed = 742\r\n",
        "torch.manual_seed(seed)\r\n",
        "env.seed(seed)\r\n",
        "random.seed(seed)\r\n",
        "np.random.seed(seed)\r\n",
        "env.action_space.seed(seed)\r\n",
        "\r\n",
        "q = QNetwork()\r\n",
        "q_target = QNetwork()\r\n",
        "q_target.load_state_dict(q.state_dict())\r\n",
        "\r\n",
        "# Load state dict from memory \r\n",
        "#q.load_state_dict(torch.load(f\"drive/MyDrive/q-models/q-300.pth\"))\r\n",
        "#q_target.load_state_dict(torch.load(f\"drive/MyDrive/q-models/q_target-300.pth\"))\r\n",
        "####\r\n",
        "\r\n",
        "memory = PrioritizedReplayBuffer()\r\n",
        "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\r\n",
        "\r\n",
        "frame_idx = 0 # set to where colab crashes, else 0  \r\n",
        "score = 0.0 \r\n",
        "marking = [] \r\n",
        "replay_initial = 10000\r\n",
        "\r\n",
        "# Apply wrappers\r\n",
        "env = NoopResetEnv(env)\r\n",
        "env = GrayScaleObservation(env) \r\n",
        "env = ResizeObservation(env, (84,84)) \r\n",
        "env = FrameStack(env, num_stack=4)\r\n",
        "env = FireResetEnv(env)  \r\n",
        "\r\n",
        "for n_episode in range(int(1e32)):\r\n",
        "    s = env.reset() \r\n",
        "    done = False \r\n",
        "    score = 0.0 \r\n",
        "\r\n",
        "    while True: \r\n",
        "        frame_idx += 1 \r\n",
        "        epsilon = epsilon_by_frame(frame_idx) # linear annealing from 100% to 1%\r\n",
        "        s = torch.from_numpy(np.moveaxis(s, [0,1,2,3], [1,2,3,0])).float()\r\n",
        "        a = q.sample_action(s, epsilon)\r\n",
        "\r\n",
        "        s_prime, r, done, info = env.step(a) \r\n",
        "        done_mask = 0.0 if done else 1.0 \r\n",
        "        memory.put((s,a,r/100.0,(torch.from_numpy(np.moveaxis(s_prime, [0,1,2,3], [1,2,3,0])).float()), done_mask))\r\n",
        "        \r\n",
        "        s = s_prime \r\n",
        "        score += r \r\n",
        "\r\n",
        "        if done:\r\n",
        "          break\r\n",
        "\r\n",
        "        if memory.size() > replay_initial and frame_idx % 10 == 0: # Train/learn every 10 frames to speed up training. Ideally train on every frame\r\n",
        "            beta = beta_by_frame(frame_idx)\r\n",
        "            train(q, q_target, memory, optimizer, beta)\r\n",
        "        \r\n",
        "        if frame_idx % 1000 == 0:\r\n",
        "            q_target.load_state_dict(q.state_dict())\r\n",
        "        \r\n",
        "    \r\n",
        "    # do not change lines 44-48 here, they are for marking the submission log\r\n",
        "    marking.append(score)\r\n",
        "    if n_episode%100 == 0:\r\n",
        "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\r\n",
        "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\r\n",
        "        marking = []\r\n",
        "    \r\n",
        "    if n_episode % 10 == 0 and save_model == True:\r\n",
        "        torch.save(q.state_dict(), f\"drive/MyDrive/q-models/q-{n_episode}.pth\")\r\n",
        "        torch.save(q_target.state_dict(), f\"drive/MyDrive/q-models/q_target-{n_episode}.pth\")\r\n",
        "\r\n",
        "    if n_episode%print_every==0 and n_episode!=0:\r\n",
        "        print(\"episode: {}, score: {:.1f}, epsilon: {:.2f}, beta: {:.2f}, frame_idx : {:.1f}\".format(n_episode, score, epsilon, beta, frame_idx))\r\n",
        "          "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "marking, episode: 0, score: 100.0, mean_score: 100.00, std_score: 0.00\n",
            "episode: 5, score: 500.0, epsilon: 0.45, beta: 0.40, frame_idx : 241145.0\n",
            "episode: 10, score: 0.0, epsilon: 0.45, beta: 0.55, frame_idx : 245680.0\n",
            "episode: 15, score: 0.0, epsilon: 0.44, beta: 0.55, frame_idx : 249464.0\n",
            "episode: 20, score: 100.0, epsilon: 0.44, beta: 0.55, frame_idx : 252913.0\n",
            "episode: 25, score: 350.0, epsilon: 0.43, beta: 0.55, frame_idx : 256716.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJtSADWhLODg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}